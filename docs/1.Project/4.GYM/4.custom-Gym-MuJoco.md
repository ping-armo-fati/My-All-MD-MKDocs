# Gym-MuJoco

## 参考

### 文档

- [MuJoco](https://mujoco.readthedocs.io/en/latest/programming/simulation.html)

- [Gymnasium](https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/#sphx-glr-tutorials-gymnasium-basics-environment-creation-py)

- [Gymnasium-mujoco-robotics](https://robotics.farama.org/envs/MaMuJoCo/ma_ant/)

- 环境自定义
  - https://blog.csdn.net/qq_33446100/article/details/118249795

  - 强化学习的算法库
    - https://blog.csdn.net/qq_33446100/article/details/119089724?spm=1001.2014.3001.5502

  - 源码地址 https://github.com/openai/gym/blob/master/gym/core.py



### 教程

- 极简环境及注册

https://www.youtube.com/watch?v=ZxXKISVkH6Y&t=3s

- gym下-官方参考

![corepy的位置](../../assets/107_core.py-位置.png)



- Docs

![Gym教程文档](../../assets/108_Gym教程文档.webp)

- Agent

  - random_agent.py运行一个随机的智能体对象；

  - cem.py 使用交叉熵方法运行一个确定的智能体对象；

  - keyboard_agent.py 将自己作为智能体进行测试，将环境名称作为命令行参数传递。

    （scripts暂时没看明白用来做什么的。）

- 保存视频

  ```python
  from gym import wrappers
  from time import time # just to have timestamps in the files
  env = gym.make(ENV_NAME)
  env = wrappers.Monitor(env, './videos/' + str(time()) + '/')
  ```


# Gym-ENV-MuJoCo-Stable baselines 3

## 步骤

### 环境准备

- 安装

```shell
pip install stable-baselines3
```

- 阅读软件包

路径：`/home/ypq3/anaconda3/envs/Stone/lib/python3.10/site-packages/stable_baselines3`

- 封装程序包：

  - 生成:`requirement.txt`

    - Conda-环境包装

    ```sh
       conda activate myenv
       conda env export > environment.yml
       conda list --export > requirements.txt
       conda env create -f environment.yml
       conda create --name myenv --file requirements.txt
    ```

    

  - Pycharm：Tools ->Sync Python Requirements（同步python要求）
  - pipreqs

    
  ```shell
  # 安装
  pip install pipreqs
  # 在项目根目录下运行:
  pipreqs ./ --encoding=utf8
  ```


### Gym

- Demo

```python
import gymnasium as gym
env = gym.make("LunarLander-v2", render_mode="human")
observation, info = env.reset()
for _ in range(1000):
    action = env.action_space.sample()  # agent policy that uses the observation and info
    observation, reward, terminated, truncated, info = env.step(action)

    if terminated or truncated:
        observation, info = env.reset()
env.close()
```

- Env-custom

  - Docs

  - Tutorials

  - copycat

    - 模板

    ```python
    import gym
    from gym import spaces
    
    class CustomEnv(gym.Env):
        """Custom Environment that follows gym interface"""
        def __init__(self, arg1, arg2, ...):
            super(CustomEnv, self).__init__()
            self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)
            self.observation_space = spaces.Box(low=0, high=255,
                                                shape=(N_CHANNELS, HEIGHT, WIDTH), dtype=np.uint8)
        def step(self, action):
            ...
            return observation, reward, done, info
        def reset(self):
            ...
            return observation  # reward, done, info can't be included 
        def render(self, mode="human"):
            ...
    ```

    - 游戏环境

    ```python
    import numpy as np
    import gym
    from gym import spaces
    
    
    class GoLeftEnv(gym.Env):
      """
      这是一个让智能体学习一直向左走的 1D grid 环境 
      """
      metadata = {'render.modes': ['console']}
      LEFT = 0
      RIGHT = 1
    
      def __init__(self, grid_size=10):
        super(GoLeftEnv, self).__init__()
    
        # 1D-grid 的大小
        self.grid_size = grid_size
        # agent 初始化在 grid 的最右边
        self.agent_pos = grid_size - 1
    
        # 定义 action  observation 
        # 离散行为空间: left、 right
        n_actions = 2
        self.action_space = spaces.Discrete(n_actions)
        # 观测是智能体现在的位置
        self.observation_space = spaces.Box(low=0, high=self.grid_size,
                                            shape=(1,), dtype=np.float32)
    
      def reset(self):
        """
        Important: 观测必须是一个 np.array
        :return: (np.array) 
        """
        # Initialize the agent at the right of the grid
        self.agent_pos = self.grid_size - 1
        # here we convert to float32 to make it more general (in case we want to use continuous actions)
        return np.array([self.agent_pos]).astype(np.float32)
    
      def step(self, action):
        if action == self.LEFT:
          self.agent_pos -= 1
        elif action == self.RIGHT:
          self.agent_pos += 1
        else:
          raise ValueError("Received invalid action={} which is not part of the action space".format(action))
        # 如果走到边缘就不能继续走了
        self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)
        # 如果走到最左边代表结束了
        done = bool(self.agent_pos == 0)
        # 走到最左边就给一个正的 reward
        reward = 1 if self.agent_pos == 0 else 0
        # 目前没有需要额外输出的信息
        info = {}
        return np.array([self.agent_pos]).astype(np.float32), reward, done, info
    
      def render(self, mode='console'):
        # 在命令行中渲染
        if mode != 'console':
          raise NotImplementedError()
        # agent is represented as a cross, rest as a dot
        print("." * self.agent_pos, end="")
        print("x", end="")
        print("." * (self.grid_size - self.agent_pos))
    
      def close(self):
        pass
    ```

    实例化代码

    ```python
    from stable_baselines3 import PPO, A2C # DQN coming soon
    from stable_baselines3.common.env_util import make_vec_env
    
    # 构建环境
    env = GoLeftEnv(grid_size=10)
    env = make_vec_env(lambda: env, n_envs=1)
    # 训练智能体
    model = A2C('MlpPolicy', env, verbose=1).learn(5000)
    # Test the trained agent
    obs = env.reset()
    n_steps = 20
    for step in range(n_steps):
      action, _ = model.predict(obs, deterministic=True)
      print("Step {}".format(step + 1))
      print("Action: ", action)
      obs, reward, done, info = env.step(action)
      print('obs=', obs, 'reward=', reward, 'done=', done)
      env.render(mode='console')
      if done:
        # Note that the VecEnv resets automatically
        # when a done signal is encountered
        print("Goal reached!", "reward=", reward)
        break
    ```

    

- MuJoCo-src-read

  - 环境观察：
    - 可视化：坐标系、接触力、相机切换镜头、截屏、透视、隐藏菜单
    - 位于body下：
    - ``       <camera name="track" mode="trackcom" pos="0 -3 0.3" xyaxes="1 0 0 0 0 1"/>``
  - 类的继承架构
    - Ant-V4 -> MujocoEnv ->GymEnv

- SB-3

  - Demo-cart pole -gym
  - SB-3 src reading
    - Common
    - single algorithm
    - DDPG
      - 中级错题家
      - CSDN
  - Office Docs

- Mujoco-env

  - MyEnvStone
  - RL-Knowledge-
  - Gym-customer-env
  - DDPG-sb3

  - CUDA-PyTorch-issac
    - [PyTorch documentation — PyTorch 2.2 documentation](https://pytorch.org/docs/stable/index.html)
    - [主页 - PyTorch中文文档 (pytorch-cn.readthedocs.io)](https://pytorch-cn.readthedocs.io/zh/latest/)
  - 并行CPU
  - MuJoCo-JAX
  - TensorBoard

## TensorBoard

- Official

[Tensorboard Integration — Stable Baselines3 2.3.2 documentation (stable-baselines3.readthedocs.io)](https://stable-baselines3.readthedocs.io/en/master/guide/tensorboard.html)

- Install
- Usage

```python
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter(log_dir="your_log_dir")

from stable_baselines3 import PPO

model = PPO("MlpPolicy", "CartPole-v1", tensorboard_log=writer)
model.learn(total_timesteps=10000)
```

- Launch

```shell
tensorboard --logdir your_log_dir
```

- 浏览器地址：`localhost:6006`
