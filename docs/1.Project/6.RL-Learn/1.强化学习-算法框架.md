# 强化学习训练策略

## 算法库

- [强化学习开源框架整理 ](https://zhuanlan.zhihu.com/p/582396276)
- SB3
- TF-Agents

## 环境库

- Issac Gym
- 自定义环境参考-待办
  - 环境构建
  - 

## 分层强化学习

- 论文： HyAR
  - [HyAR：通过混合动作表示解决离散-连续动作的强化学习问题 ](https://zhuanlan.zhihu.com/p/497347569)
  - name：HYAR: ADDRESSING DISCRETE-CONTINUOUS ACTION REINFORCEMENT LEARNING VIA HYBRID ACTION REPRESENTATION
  -  备注：如何画出那么好的训练曲线

## 硬件训练

- 总体软件包
  - Issac Gym CUDA Torch Board



## 算法库：RLlib

- [RLlib 入门 ](https://docs.ray.io/en/latest/rllib/rllib-training.html)

### 学习及实操

- Install

```shell
pip install "ray[rllib]" tensorflow
rllib train --algo DQN --env CartPole-v1 --stop '{"training_iteration": 30}'
```

- Board

```shell
tensorboard --logdir=~/ray_results
```

- Usage

```python
from ray.rllib.algorithms.ppo import PPOConfig
from ray.tune.logger import pretty_print


algo = (
    PPOConfig()
    .env_runners(num_env_runners=1)
    .resources(num_gpus=0)
    .environment(env="CartPole-v1")
    .build()
)

for i in range(10):
    result = algo.train()
    print(pretty_print(result))

    if i % 5 == 0:
        checkpoint_dir = algo.save().checkpoint.path
        print(f"Checkpoint saved in directory {checkpoint_dir}")
```

- Gym-Env

```python
import gymnasium as gym

from ray.rllib.algorithms.ppo import PPOConfig

env_name = "CartPole-v1"
env = gym.make(env_name)
algo = PPOConfig().environment(env_name).build()

episode_reward = 0
terminated = truncated = False
obs, info = env.reset()

while not terminated and not truncated:
    action = algo.compute_single_action(obs)
		obs, reward, terminated, truncated, info = env.step(action)
    episode_reward += reward
```

- Complex Space

```python
from gymnasium.spaces import Dict, Tuple, Box, Discrete, MultiDiscrete

from ray.tune.registry import register_env
from ray.rllib.connectors.env_to_module import (
    AddObservationsFromEpisodesToBatch,
    FlattenObservations,
    WriteObservationsToEpisodes,
)
from ray.rllib.examples.envs.classes.multi_agent import (
    MultiAgentNestedSpaceRepeatAfterMeEnv,
)
from ray.rllib.examples.envs.classes.nested_space_repeat_after_me_env import (
    NestedSpaceRepeatAfterMeEnv,
)
from ray.rllib.utils.test_utils import (
    add_rllib_example_script_args,
    run_rllib_example_script_experiment,
)
from ray.tune.registry import get_trainable_cls


# Read in common example script command line arguments.
parser = add_rllib_example_script_args(default_timesteps=200000, default_reward=-500.0)


if __name__ == "__main__":
    args = parser.parse_args()

    # Define env-to-module-connector pipeline for the new stack.
    def _env_to_module_pipeline(env):
        return [
            AddObservationsFromEpisodesToBatch(),
            FlattenObservations(multi_agent=args.num_agents > 0),
            WriteObservationsToEpisodes(),
        ]

    # Register our environment with tune.
    if args.num_agents > 0:
        register_env(
            "env",
            lambda c: MultiAgentNestedSpaceRepeatAfterMeEnv(
                config=dict(c, **{"num_agents": args.num_agents})
            ),
        )
    else:
        register_env("env", lambda c: NestedSpaceRepeatAfterMeEnv(c))

    # Define the AlgorithmConfig used.
    base_config = (
        get_trainable_cls(args.algo)
        .get_default_config()
        .environment(
            "env",
            env_config={
                "space": Dict(
                    {
                        "a": Tuple(
                            [Dict({"d": Box(-15.0, 3.0, ()), "e": Discrete(3)})]
                        ),
                        "b": Box(-10.0, 10.0, (2,)),
                        "c": MultiDiscrete([3, 3]),
                        "d": Discrete(2),
                    }
                ),
                "episode_len": 100,
            },
        )
        .env_runners(env_to_module_connector=_env_to_module_pipeline)
        # No history in Env (bandit problem).
        .training(
            gamma=0.0,
            lr=0.0005,
            model=(
                {} if not args.enable_new_api_stack else {"uses_new_env_runners": True}
            ),
        )
    )

    # Add a simple multi-agent setup.
    if args.num_agents > 0:
        base_config.multi_agent(
            policies={f"p{i}" for i in range(args.num_agents)},
            policy_mapping_fn=lambda aid, *a, **kw: f"p{aid}",
        )

    # Fix some PPO-specific settings.
    if args.algo == "PPO":
        base_config.training(
            # We don't want high entropy in this Env.
            entropy_coeff=0.00005,
            num_sgd_iter=4,
            vf_loss_coeff=0.01,
        )

    # Run everything as configured.
    run_rllib_example_script_experiment(base_config, args)
```



## Option-Critic

- 官方源码库
  - [official-four-room-1](https://github.com/jeanharb/option_critic)
  - [pytorch-four-room](https://github.com/lweitkamp/option-critic-pytorch)
  - 
- Git-Hub其他库
- 论文-概念笔记
  - 好
  - https://blog.csdn.net/Fearless_Sun/article/details/134248964
  - https://blog.csdn.net/qq_40206371/article/details/125265655
  - 推导
  - https://zhuanlan.zhihu.com/p/430471198
  - 原文阅读
    - https://zhuanlan.zhihu.com/p/217811126
    - https://zhuanlan.zhihu.com/p/58993559
    - https://zhuanlan.zhihu.com/p/59211348
    - https://zhuanlan.zhihu.com/p/58895331
    - https://zhuanlan.zhihu.com/p/58119861

- Paper

  - Bacon_2017_The Option-Critic Architecture 





### 复现1

- four-room
  - [official-four-room-1](https://github.com/jeanharb/option_critic)
  - theano包，2017年停止维护。cv2最低要求python3.6
- 创建虚拟环境

> 1. **创建conda虚拟环境**：
>
>    ```bash
>    conda create -n option-critic python=3.6
>    ```
>
> 2. **激活虚拟环境**：
>
>    ```bash
>    conda activate option-critic
>    ```
>
> 3. **在PyCharm中设置虚拟环境**：
>
>    - 打开你的项目。
>    - 选择`File` -> `Settings` -> `Project: Your_Project_Name` -> `Python Interpreter`。
>    - 点击右上角的齿轮图标，然后选择`Add`。
>    - 在左侧选择`Conda Environment`，然后在右侧的`Interpreter`下拉菜单中选择你刚刚创建的`Option-C`环境。如果你找不到它，你可能需要手动输入它的路径。一般来说，conda环境的路径应该类似于`/home/username/anaconda3/envs/Option-C/bin/python`（这个路径可能会根据你的系统和conda的安装位置有所不同）。
>    - 点击`OK`。
>
> 4. **在PyCharm的终端中激活虚拟环境**：在PyCharm的终端中，你可以使用和第2步相同的命令来激活你的虚拟环境：
>
>    ```bash
>    conda activate option-critic
>    ```
>
> ​              

- ale-api

  - > from ale_python_interface import ALEInterface ModuleNotFoundError: No module named 'ale_python_interface'

  - > pip install ale-py
    >
    > from ale_py import ALEInterface

  - 下载17年前的Release

  - 根据`manu.pdf`

    - ```shell
      # install requirments
       sudo apt-get install cmake
       sudo apt-get install libsdl1.2-dev
      # compile
      cd ale_0_5
      cmake -DUSE_SDL=ON -DUSE_RLGLUE=OFF -DBUILD_EXAMPLES=ON .
      make -j 4
      # python install
      pip install .
      ```

- `cv2`

  - ```shell
    pip install opencv-python==3.4.0.12
    ```

- theano

  - ```shell
    pip install theano==0.7.0
    ```

- ROMs of atari

  - ALE运行需要雅达利的ROMs 
    - https://github.com/spragunr/deep_q_rl/issues/30
    - 
  - 去官网下载
    - https://blog.csdn.net/qq_34666857/article/details/124027874
    - [Atari 2600 VCS ROM Collection (atarimania.com)](https://www.atarimania.com/rom_collection_archive_atari_2600_roms.html)

- Fourroom 子文件夹中

  - ```shell
    pip install gym==0.1.0
    pip install dill==0.2.0
    ```

- 挂起-失败



### 复现2

- https://github.com/lweitkamp/option-critic-pytorch

- 修改`main.py`

  - ````shell
    # 修改环境版本
    cart pole -v1
    # 降低gym版本
    pip install gym==0.25.3
    ````

  - 





### 复现3

- 与复现1环境相同
- https://github.com/tdavchev/option-critic
- 暂时需要TensorFlow 搁置



### 复现4

https://github.com/alversafa/option-critic-arch?tab=readme-ov-file



### 复现5-PPOC

https://github.com/mklissa/PPOC?tab=readme-ov-file

- Proximal Policy Option-Critic algorithm

  - 安装baselines

  ```shell
  # 依赖
  sudo apt-get update && sudo apt-get install cmake libopenmpi-dev python3-dev zlib1g-dev
  # 
  ```

  - 报错

  > 下列软件包有未满足的依赖关系：
  >  libopenmpi-dev : 依赖: libibverbs-dev (>= 1.1.7) 但是它将不会被安装
  > E: 无法修正错误，因为您要求某些软件包保持现状，就是它们破坏了软件包间的依赖关系。

  - 未解决

- 使用的是baselines



### 复现6

Safe Option Critic

- https://github.com/arushijain94/SafeOptionCritic?tab=readme-ov-file

### 复现7

- https://github.com/kkhetarpal/safe_a2oc_delib?tab=readme-ov-file



### 复习8

POC prioritized-OC

- https://github.com/YuejiangLIU/prioritized_option_critic
- 

## 强化学习常用工具和库

### argparse

- 参考
  - <https://blog.csdn.net/Fan19zju/article/details/118570720>
  - https://docs.python.org/3/howto/argparse.html
  - https://www.cnblogs.com/software-develop/articles/18060482
- 例子

```python
import argparse
# 实例化
parser = argparse.ArgumentParser()
# 添加参数
parser.add_argument("-t", "--timer_name", help="timer name")
parser.add_argument("-p", "--project", help="project name")
parser.add_argument('-s', '--sparse', action='store_true', default=False, help='GAT with sparse version or not.')
parser.add_argument('-d', '--seed', type=int, default=72, help='Random seed.')
parser.add_argument('-e', '--epochs', type=int, default=10000, help='Number of epochs to train.')
# 解析参数
args = parser.parse_args()

print(args.timer_name)
print(args.sparse)
print(args.seed)
print(args.epochs)

```

### Pkl

https://blog.csdn.net/qq_41813454/article/details/136933461

## 强化学习的概念

- 关于轮次回合等等

> 1. [**迭代（Iteration）**：迭代通常指的是参数更新的次数](https://blog.csdn.net/L0_L0/article/details/123567990)[1](https://blog.csdn.net/L0_L0/article/details/123567990)[2](https://blog.csdn.net/miles_ye/article/details/104885783)[3](https://blog.csdn.net/qq_41915623/article/details/124847431)[。在深度学习中，每次迭代会对模型权重进行一次更新](https://blog.csdn.net/L0_L0/article/details/123567990)[1](https://blog.csdn.net/L0_L0/article/details/123567990)[2](https://blog.csdn.net/miles_ye/article/details/104885783)[3](https://blog.csdn.net/qq_41915623/article/details/124847431)[。例如，如果你有10000个样本，并且你的批大小（batch size）是20，那么你需要500次迭代来完成一个轮次（epoch）](https://blog.csdn.net/L0_L0/article/details/123567990)[1](https://blog.csdn.net/L0_L0/article/details/123567990)。
> 2. [**步骤（Step）**：步骤和迭代在许多情况下是可以互换的，都是指参数更新的次数](https://blog.csdn.net/L0_L0/article/details/123567990)[3](https://blog.csdn.net/qq_41915623/article/details/124847431)。
> 3. [**轮次（Epoch）**：轮次是指训练数据集中的所有样本输入模型被训练的次数](https://blog.csdn.net/L0_L0/article/details/123567990)[1](https://blog.csdn.net/L0_L0/article/details/123567990)[2](https://blog.csdn.net/miles_ye/article/details/104885783)[3](https://blog.csdn.net/qq_41915623/article/details/124847431)[。例如，如果你有10000个训练样本，这10000个样本都跑完就算一个轮次（epoch）](https://blog.csdn.net/L0_L0/article/details/123567990)[1](https://blog.csdn.net/L0_L0/article/details/123567990)[2](https://blog.csdn.net/miles_ye/article/details/104885783)[3](https://blog.csdn.net/qq_41915623/article/details/124847431)。
> 4. **运行（Runs）**：运行通常指的是整个训练过程的执行次数。例如，如果你有一个模型，并且你训练它10次，那么你就有10个运行。
> 5. [**情节（Episode）**：情节常用于强化学习，以游戏举例，例如模型训练中途或迭代结束后，玩一轮游戏（例如玩一局飞机大战）看看本局游戏能得多少奖励。无论通关还是失败，都是一个情节（episode）](https://blog.csdn.net/L0_L0/article/details/123567990)[1](https://blog.csdn.net/L0_L0/article/details/123567990)



## Actor-Critic复现

### 复现1 B站的相关资料

https://note.youdao.com/ynoteshare/index.html?id=3b8cb1b1a0a46799f5c00e51329bfba8&type=note&_time=1716723875591
