# 调参

## 参数概述

- 策略学习超参数

| 名称         | 符号          | 作用                         | 影响                            | 基准值                | 表达式 |
| ------------ | ------------- | ---------------------------- | ------------------------------- | --------------------- | ------ |
| 探索噪声     | explore_noise | 控制探索的随机性             | 越大探索越多，越小利用越多      |                       |        |
| 折扣系数     | $\gamma$      | 设置策略的远见性             | 越大，当前奖励权重越高          |                       |        |
| 软更新系数   | $\tau$        | 确定策略网络的更新速率       | 越大 更新越快                   | 0.7-0.9、（0.9-0.97） |        |
| 记忆容量     | memory_size   | 训练轨迹的储存大小           | 决定智能体的经验“多少”          |                       |        |
| 缓存容量     | batch_size    | 控制单次经验的利用容量       | 决定智能体一次回忆多少“经验”    |                       |        |
| 学习率       | Learning_rate | 控制模型在单次更新的步进程度 | 越大 更新越快 学习越快 无法收敛 |                       |        |
| 训练频率     | train_freq    | 控制经验的利用程度           | 与学习率相耦合                  |                       |        |
| 最大步数     | Max_step      | 控制环境允许的最大探索次数   |                                 |                       |        |
| 神经网络结构 | hide_n        | 单层节点数                   |                                 | 512 256 256 128 64 1  |        |
| 探索率       | $\epsilon$    | 探索与利用平衡               |                                 |                       |        |

- 训练返回值处理

  - 归一化（$Tanh$）

  > 避免探索输出靠近行动空间边缘：
  >
  > ​         actor 网络输出的行动空间值，用$Tanh$将值控制在$[-1,1]$, 此后再缩放行动获得动作数据。但策略输出的值都落于激活函数保护区，则会出现输出靠近两个边界，是由于激活函数的特点使得输出的动作趋近于无差异。
  >
  > 实操：
  >
  > ​         在参数输入网络前进行归一化处理，避免进入函数的保护区

  - 策略与奖励设置

    - 稳定策略
      - 低分惩罚
      - 信任域更新控制

    - 激进策略
      - 高分奖励

- 参数间耦合关系

  - 基准-Max_steps

  ```python
  max_steps = level x agv_trial_len（60） ≈ 2^7
  ```

  - $\gamma$:

  ```python
  q_target = r + gamma * Cri'(s_, a_, max_steps)   
  iteral 300 steps：
  		0.98 ~ 57
    	0.97 ~ 216
      0.90 ~ 475
  ```

  - memory_size 和batch size

  ```python
  memories_size = steps（60 ， 300） * batch_size 
  ```

  - 噪声

  ```python
  						        	激进策略   稳妥策略
  explore explore_noise 0.4       0.1
  policy  policy_nose   0.8       0.2
  ```

  - Leaning_rate

  ```python
  ```

  