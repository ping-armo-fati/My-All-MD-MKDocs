# Isaac

## 概述

Isaac gym是NVIDIA推出的用于强化学习的仿真环境，Isaac gym可以运行在GPU上，还并行化运行多个仿真，提高仿真效率。

- 官网

https://developer.nvidia.com/isaac-gym

- docs/install.html中有下载教程
- docs/index.html中有详细的编程教程。



## 安装

### Preview安装

- 下载

https://developer.nvidia.com/isaac-gym/download

- 创建已有的`rlgpu`环境，根目录下

```shell
./create_conda_env_rlgpu.sh
```

- 激活环境

```shell
conda activate rlgpu
```

- 进入python目录安装执行（`xxx/isaacgym/python/`）

```shell
pip install -e .
```

- :file_folder:`examples`示例文件

```shell
python joint_monkey.py
```

- 报错

https://blog.csdn.net/m0_46336568/article/details/126920033?spm=1001.2014.3001.5502

```shell
sudo apt install nvidia-driver-495
```

```shell
[Error] [carb.windowing-glfw.plugin] GLFW initialization failed.
[Error] [carb.windowing-glfw.plugin] GLFW window creation failed!
[Error] [carb.gym.plugin] Failed to create Window in CreateGymViewerInternal
```

>“Isaac”通常指的是NVIDIA公司的Isaac平台，它是一款面向机器人开发的开放式软件平台。而“isaac-gym”和“isaac-ros”则是Isaac平台中的两个不同模块。
>
>1. Gym:
>   - 区别：isaac-gym是Isaac平台中的一部分，它是基于开源的OpenAI Gym库扩展而来的，用于机器人学习和仿真环境。
>   - 作用：isaac-gym提供了用于训练和评估机器人控制算法的仿真环境，其中包括各种不同的仿真场景和任务。
>   - 部署：可以在支持NVIDIA GPU的计算机上部署isaac-gym，并通过Isaac SDK进行配置和使用。
>
>2. ROS (Robotic Operating System):
>   - 区别：isaac-ros则是Isaac平台和ROS之间的桥梁，它允许Isaac平台与ROS生态系统无缝集成，从而利用ROS的丰富功能和算法。
>   - 作用：通过isaac-ros，用户可以在Isaac平台上利用ROS的各种节点、消息、服务等功能，扩展和增强机器人应用的能力。
>   - 部署：可以在支持ROS的机器人系统上部署isaac-ros，并通过Isaac SDK进行配置和使用。
>

### 独立的训练环境安装

- 地址

https://github.com/NVIDIA-Omniverse/IsaacGymEnvs

- 解压后-注意同一环境下

```shell
pip install -e .
```

- 训练指令

```shell
python train.py task=Ant
```

- 文件架构

missing ：2664c61612b1c6f9b0724e4d281efecf-1701236096734-4.jpg

### 编写环境

```python
import isaacgym
import isaacgymenvs
import torch
# 环境中并行模型数量
num_envs = 2000
# 创建环境
envs = isaacgymenvs.make(
	seed=0, 
	task="Ant", 
	num_envs=num_envs, 
	sim_device="cuda:0",
	rl_device="cuda:0",
)
print("Observation space is", envs.observation_space)
print("Action space is", envs.action_space)
obs = envs.reset()
for _ in range(20):
	# 产生随机动作
	random_actions = 2.0 * torch.rand((num_envs,) + envs.action_space.shape, device = 'cuda:0') - 1.0
	# 与环境交互
	envs.step(random_actions)
```

- 修改模型文件-存储路径

```pyhton
def _create_envs(self, num_envs, spacing, num_per_row):
	asset_root = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../../assets')
    asset_file = "/urdf/anymal_b_simple_description/urdf/anymal.urdf"
```

- 修改奖励

```python
def compute_anymal_reward(
    # tensors
    root_states,
    commands,
    torques,
    contact_forces,
    knee_indices,
    episode_lengths,
    # Dict
    rew_scales,
    # other
    base_index,
    max_episode_length
):
    # (reward, reset, feet_in air, feet_air_time, episode sums)
    # type: (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Dict[str, float], int, int) -> Tuple[Tensor, Tensor]

    
    # Retrieves buffer for Actor root states. The buffer has shape (num_actors, 13). State for each actor root contains position([0:3]), rotation([3:7]), linear velocity([7:10]), and angular velocity([10:13]).
    # prepare quantities (TODO: return from obs ?)
    base_quat = root_states[:, 3:7]
    # 获取线性速度
    base_lin_vel = quat_rotate_inverse(base_quat, root_states[:, 7:10])
    # 获取角速度
    base_ang_vel = quat_rotate_inverse(base_quat, root_states[:, 10:13])

    # velocity tracking reward 计算误差当前状态减去上一个状态的平方
    lin_vel_error = torch.sum(torch.square(commands[:, :2] - base_lin_vel[:, :2]), dim=1)
    ang_vel_error = torch.square(commands[:, 2] - base_ang_vel[:, 2])
    
	# rew_scales["lin_vel_xy"]和rew_scales["ang_vel_z"] 是奖励系数在/cfg/task/Anymal.yaml中设置
    rew_lin_vel_xy = torch.exp(-lin_vel_error/0.25) * rew_scales["lin_vel_xy"]
 
    rew_ang_vel_z = torch.exp(-ang_vel_error/0.25) * rew_scales["ang_vel_z"]

    # torque penalty 惩罚系数
    rew_torque = torch.sum(torch.square(torques), dim=1) * rew_scales["torque"]
	# 总奖励
    total_reward = rew_lin_vel_xy + rew_ang_vel_z + rew_torque
    total_reward = torch.clip(total_reward, 0., None)
    # reset agents重置机器人参数
    reset = torch.norm(contact_forces[:, base_index, :], dim=1) > 1.
    reset = reset | torch.any(torch.norm(contact_forces[:, knee_indices, :], dim=2) > 1., dim=1)
    time_out = episode_lengths >= max_episode_length - 1  # no terminal reward for time-outs
    reset = reset | time_out

    return total_reward.detach(), reset

```

https://blog.csdn.net/weixin_44061195/article/details/131830133



## 进阶

```shell
## Isaac Gym环境安装和四足机器人模型的训练
https://blog.csdn.net/weixin_44061195/article/details/131830133
## ubuntu 18.04 搭建isaacgym学习环境，并运行legged_gym
https://blog.csdn.net/weixin_45315065/article/details/132902799
## 腿足机器人
https://leggedrobotics.github.io/legged_gym/
## 对应论文
 Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning
 https://arxiv.org/abs/2109.11978
 ## 过程
 git clone https://github.com/leggedrobotics/rsl_rl.git
cd rsl_rl && pip install -e .

git clone https://github.com/leggedrobotics/legged_gym.git
cd legged_gym && pip install -e .

# test
# 训来模型
python issacgym_anymal/scripts/train.py --task=anymal_c_flat
# 测试模型
python issacgym_anymal/scripts/play.py --task=anymal_c_flat

```

